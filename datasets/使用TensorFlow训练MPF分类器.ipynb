{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../loader\")\n",
    "# 载入 CASIA 的 MPF \n",
    "from casia.feature import CASIAFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入一些必备包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入 HDF5 数据并将其转换为训练集与测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_path = 'data/features.h5'\n",
    "mpf_dataset = CASIAFeature(hdf_path)\n",
    "# 划分数据集为训练集与测试集\n",
    "train_features = np.concatenate([features for features, _ in mpf_dataset.train_iter()])\n",
    "train_labels = np.concatenate([labels for _, labels in mpf_dataset.train_iter()])\n",
    "test_features = np.concatenate([features for features, _ in mpf_dataset.test_iter()])\n",
    "test_labels = np.concatenate([labels for _, labels in mpf_dataset.test_iter()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别获取测试集与训练集的样本数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072276, 4299331)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels), len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据的标签及其类别个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集与测试集的类别标签是相同的\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集的类别名称集\n",
    "train_class_names = set(train_labels)\n",
    "# 获取训练测试集的类别名称集\n",
    "test_class_names = set(test_labels)\n",
    "is_same_class = \"相同\" if train_class_names == test_class_names else \"不相同\"\n",
    "print(f'训练集与测试集的类别标签是{is_same_class}的')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于训练集与测试集的类别标签是相同的，所以下面可以将类别名称写作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别个数： 3755\n"
     ]
    }
   ],
   "source": [
    "class_names = test_class_names\n",
    "# 获取类别个数\n",
    "CLASS_NUM = len(class_names)\n",
    "print('类别个数：', CLASS_NUM )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将类别名称转换为 编号 -> 类别名称 的映射关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'号'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dict = dict(enumerate(class_names))\n",
    "cat_dict[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择指定类别名称列表的数据集迭代器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from casia.feature import SubCASIA\n",
    "\n",
    "class_names = ['登', '印', '枕', '孤', '美', '好', '琴', '驱', '吞', '山']\n",
    "mpf_dataset = SubCASIA(class_names, hdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计有多少个 MPF 文件（即有多少人）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = 0\n",
    "for features, labels in mpf_dataset.sub_train_iter():\n",
    "    n_train += 1\n",
    "    \n",
    "n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计有多少个样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的样本数 11500，测试集的样本数 2869\n"
     ]
    }
   ],
   "source": [
    "n_train = 0\n",
    "for features, labels in mpf_dataset.sub_train_iter():\n",
    "    n_train += len(labels) \n",
    "n_test = 0\n",
    "for features, labels in mpf_dataset.sub_test_iter():\n",
    "    n_test += len(labels)\n",
    "print(f\"训练集的样本数 {n_train}，测试集的样本数 {n_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重置 cat_dict："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = dict(enumerate(class_names))\n",
    "# 重新划分数据集为训练集与测试集\n",
    "train_features = np.concatenate([features for features, _ in mpf_dataset.sub_train_iter()])\n",
    "train_labels = np.concatenate([labels for _, labels in mpf_dataset.sub_train_iter()])\n",
    "test_features = np.concatenate([features for features, _ in mpf_dataset.sub_test_iter()])\n",
    "test_labels = np.concatenate([labels for _, labels in mpf_dataset.sub_test_iter()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '登',\n",
       " 1: '印',\n",
       " 2: '枕',\n",
       " 3: '孤',\n",
       " 4: '美',\n",
       " 5: '好',\n",
       " 6: '琴',\n",
       " 7: '驱',\n",
       " 8: '吞',\n",
       " 9: '山'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看类别字典\n",
    "cat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标签数组转换为数值型数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 index -> name 的 dict\n",
    "name2index = {cat:cat_id for cat_id, cat in cat_dict.items()}\n",
    "# 转换训练集的标签\n",
    "train_labels = np.array([name2index[cat_name] for cat_name in train_labels])\n",
    "# 转换测试集的标签\n",
    "test_labels = np.array([name2index[cat_name] for cat_name in test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 NumPy 数组转换为 TensorFlow 的迭代器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据分成批量，且打乱训练集数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数，评估函数，优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# 定义模型训练的优化方法\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# 选择衡量指标来度量模型的损失值（loss）和准确率（accuracy）。这些指标在 epoch 上累积值，然后打印出整体结果\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 `tf.GradientTape` 定义如何训练模型以及评估模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, CLASS_NUM):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(300, activation='relu')\n",
    "        self.d2 = Dense(CLASS_NUM, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "# Create an instance of the model\n",
    "CLASS_NUM = len(cat_dict)\n",
    "model = MyModel(CLASS_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练并评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7724343538284302, Accuracy: 92.5478286743164, Test Loss: 3.6418545246124268, Test Accuracy: 72.15057373046875\n",
      "Epoch 2, Loss: 0.17651645839214325, Accuracy: 97.65217590332031, Test Loss: 0.15243582427501678, Test Accuracy: 97.69954681396484\n",
      "Epoch 3, Loss: 0.019473550841212273, Accuracy: 99.5130386352539, Test Loss: 0.18716318905353546, Test Accuracy: 98.22237396240234\n",
      "Epoch 4, Loss: 0.009412109851837158, Accuracy: 99.73912811279297, Test Loss: 0.10833640396595001, Test Accuracy: 98.84977722167969\n",
      "Epoch 5, Loss: 0.010610833764076233, Accuracy: 99.83478546142578, Test Loss: 0.3134274482727051, Test Accuracy: 96.09619903564453\n",
      "Epoch 6, Loss: 0.00728351715952158, Accuracy: 99.81739044189453, Test Loss: 0.14625050127506256, Test Accuracy: 98.43151092529297\n",
      "Epoch 7, Loss: 0.0035412530414760113, Accuracy: 99.88695526123047, Test Loss: 0.16272664070129395, Test Accuracy: 98.08295440673828\n",
      "Epoch 8, Loss: 0.00792350061237812, Accuracy: 99.86956787109375, Test Loss: 0.12328996509313583, Test Accuracy: 98.7103500366211\n",
      "Epoch 9, Loss: 0.014404850080609322, Accuracy: 99.81739044189453, Test Loss: 0.2505173683166504, Test Accuracy: 97.5252685546875\n",
      "Epoch 10, Loss: 0.03290843218564987, Accuracy: 99.46086883544922, Test Loss: 0.23766307532787323, Test Accuracy: 97.56012725830078\n",
      "Epoch 11, Loss: 0.03684164583683014, Accuracy: 99.46086883544922, Test Loss: 0.45846039056777954, Test Accuracy: 95.92192077636719\n",
      "Epoch 12, Loss: 0.03231879323720932, Accuracy: 99.5999984741211, Test Loss: 0.18834072351455688, Test Accuracy: 98.43151092529297\n",
      "Epoch 13, Loss: 0.020721497014164925, Accuracy: 99.64347839355469, Test Loss: 0.18910744786262512, Test Accuracy: 98.57093048095703\n",
      "Epoch 14, Loss: 0.015636593103408813, Accuracy: 99.73043060302734, Test Loss: 0.3138055205345154, Test Accuracy: 97.03730010986328\n",
      "Epoch 15, Loss: 0.02274303510785103, Accuracy: 99.704345703125, Test Loss: 0.33473747968673706, Test Accuracy: 97.2115707397461\n",
      "Epoch 16, Loss: 0.018466291949152946, Accuracy: 99.77391052246094, Test Loss: 0.2234416902065277, Test Accuracy: 98.60578918457031\n",
      "Epoch 17, Loss: 0.021789859980344772, Accuracy: 99.73912811279297, Test Loss: 0.31134605407714844, Test Accuracy: 97.94353485107422\n",
      "Epoch 18, Loss: 0.012784793972969055, Accuracy: 99.75652313232422, Test Loss: 0.1742866188287735, Test Accuracy: 98.7103500366211\n",
      "Epoch 19, Loss: 0.01786620542407036, Accuracy: 99.84347534179688, Test Loss: 0.8432843685150146, Test Accuracy: 94.2140121459961\n",
      "Epoch 20, Loss: 0.021432138979434967, Accuracy: 99.66956329345703, Test Loss: 0.14604856073856354, Test Accuracy: 99.05890655517578\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result()*100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
